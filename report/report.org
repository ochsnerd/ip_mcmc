#+TITLE: Markov Chain Monte Carlo for Inverse Problems


#+LATEX_HEADER: \usepackage{physics}
#+LATEX_HEADER: \usepackage{dsfont}

#+LATEX_HEADER: \newcommand{\C}{{\mathcal{C}}}
#+LATEX_HEADER: \newcommand{\I}{{\mathcal{I}}}
#+LATEX_HEADER: \newcommand{\R}{{\mathbb{R}}}
#+LATEX_HEADER: \newcommand{\G}[1]{{\mathcal{G} \left( #1 \right)}}
#+LATEX_HEADER: \newcommand{\N}[2]{\mathcal{N}\left(#1,#2\right)}

* TODO Meta                                                        :noexport:
** TODO Can I get code execution to work here for the results? (-> DIY jupyter I guess)
** TODO Can I embed svgs?
** DONE Create/Link to bibtex file
** DONE Tests for pCN prop/acc
** TODO Write down what I've done so far
*** TODO What's up with the ac of pCN?
*** DONE Write up/insert plots
** TODO Theory: What is an infinite-dimensional Gaussian?
*** Some definition about random fields blabla in cotter
*** What about BB stuart?
*** What about the internet?
** TODO Code BB Stuart Example 2.1
** TODO Code BB Stuart Example 2.2
** TODO Read Geophysics example


* Theory
** Papers
*** Stuart et al: Inverse Problems: A Bayesian Perspective [[cite:stuart_inverse_2010]]
    Theoretical Background
**** Notation
     Central equation:
     $$y = \G{u} + \eta$$
     with:
     - $y \in \R^q$: data
     - $u \in \R^n$: IC ("input to mathematical model")
     - $\G{\cdot} :\R^n \to \R^q$: observation operator
     - $\eta$: mean zero RV, observational noise (a.s. $\eta \sim \N{0}{\C}$)
*** Cotter et al: MCMC for functions [[cite:cotter_mcmc_2013]]
    Implementation, MCMC in infinite dimensions
*** Schneider et al: Earth System Modeling 2.0  [[cite:schneider_earth_2017]]
    Example for MCMC on ODE
** Small results
*** Bayes' Formula & Radon-Nikodym Derivative
    Bayes' Formula is stated using the Radon-Nikodym Derivative in both [[cite:cotter_mcmc_2013]] and [[cite:stuart_inverse_2010]]:
    $$\dv{\mu}{\mu_0} \propto \text{L}(u),$$
    where $\text{L}(u)$ is the likelihood.

    Write the measures as $\dd \mu = \rho(u)\dd u$ and $\dd \mu_0 = \rho_0(u)\dd u$ with respect
    to the standard Lesbesgue measure. Then we have
    $$
    \int f(u) \rho(u) \dd u =
    \int f(u) \dd \mu(u) =
    \int f(u) \dv{\mu(u)}{\mu_0(u)} \dd \mu_0 =
    \int f(u) \dv{\mu(u)}{\mu_0(u)} \rho_0(u) \dd u,
    $$
    provided that $\dd \mu$, $\dd \mu_0$ and $f$ are nice enough (which they are since we're working
    with Gaussians). This holds for all test functions $f$, so it must hold pointwise:
    $$ \dv{\mu(u)}{\mu_0(u)} = \frac{\rho(u)}{\rho_o(u)}.$$

    Using this we recover the more familiar formulation of Bayes' formula:
    $$\frac{\rho(u)}{\rho_o(u)} \propto \text{L}(u).$$

*** Acceptance Probability for Metropolis-Hastings
    A Markov process with transition probabilities $t(y|x)$ has a stationary distribution $\pi(x)$.
    - The _existence_ of $\pi(x)$ follows from /detailed balance/:
      $$\pi(x)t(y|x) = \pi(y)t(x|y).$$
      Detailed balance is sufficient but not necessary for the existence of a stationary distribution.
    - _Uniqueness_ of $\pi(x)$ follows from the Ergodicity of the Markov process. For a Markov
      processto be Ergodic it has to:
      - not return to the same state in a fixed interval
      - reach every state from every other state in finite time
    
    The Metropolis-Hastings algorithm constructs transition probabilities $t(y|x)$ such that the
    two conditions above are satisfied and that $\pi(x) = P(x)$, where $P(x)$ is the distribution
    we want to sample from.

    Rewrite detailed balance as
    $$\frac{t(y|x)}{t(x|y)} = \frac{P(y)}{P(x)}.$$
    Split up the transition probability into proposal $g(y|x)$ and acceptance $a(y,x)$. Then detailed
    balance requires
    $$\frac{a(y,x)}{a(x,y)} = \frac{P(y)g(x|y)}{P(x)g(y|x)}.$$
    Choose
    $$a(y,x) = \min\left\{1, \frac{P(y)g(x|y)}{P(x)g(y|x)}\right\}$$
    to ensure that detailed balance is always satisfied. Choose $g(y|x)$ such that ergodicity
    is fulfilled.

    If the proposal is symmetric ($g(y|x) = g(x|y)$), then the acceptance takes the simpler form
    #+NAME: eqn:acceptance_simple
    \begin{equation}
    a(y,x) = \min\left\{1, \frac{P(y)}{P(x)}\right\}.
    \end{equation}

    Since the target distribution $P(x)$ only appears as a ratio, normalizing factors can be ignored.

*** Potential for Bayes'-MCMC when sampling from analytic distributions
    How can we use formulations of Metropolis-Hastings-MCMC algorithms designed to sample from
    posteriors when want to sample from probability distribution with an easy analytical expression?

    Algorithms for sampling from a posterior sample from
    $$\rho(u) \propto \rho_0(u) \exp(-\Phi(u)),$$
    where $\rho_0$ is the prior and $\exp(-\Phi(u))$ is the likelihood. Normally, we have an
    efficient way to compute the likelihood.

    When we have an efficient way to compute the posterior $\rho$ and we want to sample from it,
    the potential to do that is:
    $$\Phi(u) = \ln(\rho_0(u)) - \ln(\rho(u)),$$
    where an additive constant from the normalization was omitted since only potential differences
    are relevant.

    When working with a Gaussian prior $\N{0}{\C}$, the potential takes the form
    $$\Phi(u) = -\ln{\rho(u)} - \frac{1}{2} \norm{\C^{-1/2}u}^2.$$

    When inserting this into the acceptance probability for the standard random walk MCMC given
    in formula (1.2) in [[cite:cotter_mcmc_2013]], the two Gaussian-expressions cancel, as do the
    logarithm and the exponentiation, leaving the simple acceptance described in [[eqn:acceptance_simple]].

    This cancellation does not happen when using the pCN-Acceptance probablity. This could
    explain the poorer performance of pCN when directly sampling a probablity distribution.

*** Acceptance Probabilities for different MCMC Proposers
    Start from Bayes' formula and rewrite the likelyhood $\text{L}(u)$ as $\exp(-\Phi(u))$ for
    a positive scalar function $\Phi$ called the potential:
    $$\frac{\rho(u)}{\rho_o(u)} \propto \exp(\Phi(u)).$$
    Assuming our prior to be a Gaussian ($\mu_0 \sim \N{0}{\C}$).

    (IS WRITING $\rho_0(u) \propto \exp(-\frac{1}{2} u^TC^{-1}u)$ ASSUMING FINITE DIMENSIONS? WHAT
    ABOUT $\rho_0(u) \propto \exp(-\frac{1}{2} \norm{C^{-1/2}u}^2)$? I assume the former is not,
    for $C$ to be a proper covariance operator it should be invertible. But taking the square root
    is probably not always well defined for infinite dimensions (so the latter one is problematic))

    Then $$\rho(u) \propto \exp\left( -\Phi(u) + \frac{1}{2} \norm{C^{-1/2}u}^2 \right),$$
    since $u^T C^{-1} u = (C^{-1/2} u)^T(C^{-1/2} u) = \langle C^{-1/2}u, C^{-1/2}u \rangle = \norm{C^{-1/2} u}^2$,
    where in the first equality we used $C$ being symmetric.

    This is formula (1.2) in [[cite:cotter_mcmc_2013]] and is used in the acceptance probability for
    the standard random walk (see also [[Acceptance Probability for Metropolis-Hastings][Acceptance Probability for Metropolis-Hastings]])

    $\C^{-1/2}u$ makes problems in infinite dimensions.

    Todo: Why exactly is the second term (from the prior) cancelled when doing pCN?
*** Different formulations of multivariate Gaussians
    THIS WHOLE SECTION ASSUMES FINITE DIMENSIONS

    Is an RV $\xi \sim \N{0}{C}$ distributed the same as $C^{1/2}\xi_0$, with $\xi_0 \sim \N(0, \I)$?

    Is $C^{1/2}\exp(\frac{1}{2} x^Tx) = \exp(\frac{1}{2} x^T C^{-1} x)$ ?

    From wikipedia: Affine transformation $Y = c + BX$ for $X \sim \N{\mu}{\Sigma}$ is also a Gaussian
    $Y \sim \N{c + B\mu}{B\Sigma B^T}$. In our case $X \sim \N{0}{I}$, so $Y \sim \N{0}{C^{1/2}\I {C^{1/2}}^{T}} = \N{0}{C}$,
    since the covariance matrix is positive definite, which means it's square root is also positive definite
    and thus symmetric.

* Implementation
** Framework/Package Structure
   The framework is designed to support an easy use case:
   #+BEGIN_SRC python
   proposer = StandardRWProposer(beta=0.25, dims=1)
   accepter = AnalyticAccepter(my_distribution)
   rng = np.random.default_rng(42)
   sampler = MCMCSampler(rw_proposer, accepter, rng)

   samples = sampler.run(x_0=0, n_samples=1000)
   #+END_SRC

   There is only one source of randomness, shared among all classes and supplied by the user.
   This facilitates reproducability.

   Tests are done with ~pytest~.
*** Distributions
    A class for implementing probability distributions.
    #+BEGIN_SRC python
    class DistributionBase(ABC):
        @abstractmethod
        def sample(self, rng):
            """Return a point sampled from this distribution"""
            ...
    #+END_SRC
    
    The most important realisation is the ~GaussianDistribution~, used
    in the proposers.

    #+BEGIN_SRC python    
    class GaussianDistribution(DistributionBase):
        def __init__(self, mean=0, covariance=1):
            ...

        def sample(self, rng):
            ...

        def apply_covariance(self, x):
            ...

        def apply_sqrt_covariance(self, x):
            ...

        def apply_precision(self, x):
            ...

        def apply_sqrt_precision(self, x):
            ...
    #+END_SRC

    The design of this class is based on the implementation in [[http://muq.mit.edu/master-muq2-docs/CrankNicolson_8py_source.html][muq2]]. The ~precision~ / ~sqrt_precision~
    is implemented through a Cholesky decomposition, computed in the constructor. This makes
    applying them pretty fast ($\mathcal{O}(n^2)$).

    At the moment the there is one class for both scalar and multivariate Gaussians. This
    introduces some overhead as it has to work with both ~float~ and ~np.array~. Maybe two
    seperate classes would be better.
*** Proposers

    Propose a new state $v$ based on the current one $u$.

    #+BEGIN_SRC python
    class ProposerBase(ABC):
        @abstractmethod
        def __call__(self, u, rng):
            ...
    #+END_SRC

**** StandardRWProposer

     Propose a new state as
     $$v = u + \sqrt{2\delta} \xi,$$
     with either $\xi \sim \N{0}{\I}$ or $\xi \sim \N{0}{\C}$ (see section 4.2 in [[cite:cotter_mcmc_2013]]).

     This leads to a well-defined algorithm in finite dimensions.
     This is not the case when working on functions (as described in section 6.3 in [[cite:cotter_mcmc_2013]])

**** pCNProposer

     Propose a new state as
     $$v = \sqrt{1-\beta^2} u + \beta \xi,$$
     with $\xi \sim \N{0}{\C}$ and $\beta = \frac{8\delta}{(2+\delta)^2} \in [0,1]$
     (see formula (4.8) in [[cite:cotter_mcmc_2013]]).

     This approach leads to an improved algorithm (quicker decorrelation in finite dimensions,
     nicer properties for infinite dimensions)(see sections 6.2 + 6.3 in [[cite:cotter_mcmc_2013]]).

     The wikipedia-article on the Cholesky-factorization mentions the use-case of obtaining a
     correlated sample from an uncorrelated one by the Cholesky-factor. This is not implemented here.
*** Accepters

    Given a current state $u$ and a proposed state $v$, decide if the new state is accepted or rejected.

    For sampling from a distribution $P(x)$, the acceptance probability for a symmetric proposal is
    $a = \text{min}\{1, \frac{P(v)}{P(u)}\}$
    (see [[Acceptance Probability for Metropolis-Hastings]])

    #+BEGIN_SRC python
    class ProbabilisticAccepter(AccepterBase):
        def __call__(self, u, v, rng):
            """Return True if v is accepted"""
            a = self.accept_probability(u, v)
            return a > rng.random()

        @abstractmethod
        def accept_probability(self, u, v):
            ...
    #+END_SRC

**** AnalyticAccepter

     Used when there is an analytic expression of the desired distribution.

    #+BEGIN_SRC python
    class AnalyticAccepter(ProbabilisticAccepter):
        def accept_probability(self, u, v):
            return self.rho(v) / self.rho(u)
    #+END_SRC

**** StandardRWAccepter

     Based on formula (1.2) in [[cite:cotter_mcmc_2013]]:
     $$a = \text{min}\{1, \exp(I(u) - I(v))\},$$ with
     $$I(u) = \Phi(u) + \frac{1}{2}\norm{\C^{-1/2}u}^2$$.

     See also [[Acceptance Probabilities for different MCMC Proposers]].

**** pCNAccepter

     Works together with the [[pCNProposer][pCNProposer]] to achieve the simpler expression for the acceptance
     $$a = \text{min}\{1, \exp(\Phi(u) - \Phi(v))\}.$$

**** CountedAccepter

     Stores and forwards calls to an "actual" accepter. Counts calls and accepts and is used for
     calculating the acceptance ratio.
    
*** Sampler

    The structure of the sampler is quite simple, since it can rely heavily on the functionality
    provided by the Proposers and Accepters.

    #+BEGIN_SRC python
    class MCMCSampler:
        def __init__(self, proposal, acceptance, rng):
            ...

        def run(self, u_0, n_samples, burn_in=1000, sample_interval=200):
            ...

        def _step(self, u, rng):
            ...
    #+END_SRC

** Results
*** Analytic sampling from a bimodal Gaussian
**** Setup

     Attempting to recreate the "Computational Illustration" from [[cite:cotter_mcmc_2013]]. They use,
     among other algorithms, pCN to sample from a 1-D bimodal Gaussian
     $$\rho \propto (\N{3}{1} + \N{-3}{1}) \mathds{1}_{[-10,10]}.$$
     Since the density estimation framework for a known distribution is not quite clear to me from
     the paper, I don't expect to perfectly replicate their results.

     They use a formulation of the prior based on the Karhunen-Lo√©ve Expansion that doesn't make
     sense to me in the 1-D setting (how do I sum infinite eigenfunctions of a scalar?).

     The potential for density estimation described in section is also not clear to me (maybe for
     a similar reason? What is $u$ in the density estimate case?).

     I ended up using a normal $\N{0}{1}$ as a prior and the potential described [[Potential for Bayes'-MCMC when sampling from analytic distributions][before]], and
     compared the following samplers:
     - (1) [[StandardRWProposer][~StandardRWProposer~]] ($\delta=0.25$) + [[AnalyticAccepter][~AnalyticAccepter~]]
     - (2) [[StandardRWProposer][~StandardRWProposer~]] ($\delta=0.25$) + [[StandardRWAccepter][~StandardRWAccepter~]] 
     - (3) [[pCNProposer][~pCNProposer~]] ($\beta=0.25$) + [[pCNAccepter][~pCNAccepter~]] 

     The code is in [[file:scripts/analytic.py][~analytic.py~]].

**** Result

     All three samplers are able to reproduce the target density [[fig:hist_analytic]] [[fig:hist_rw]] [[fig:hist_rw]].

     #+CAPTION: analytic
     #+NAME: fig:hist_analytic
     [[./figures/analytic_bimodal_density.png]]
     #+CAPTION: standard rw
     #+NAME: fig:hist_rw
     [[./figures/standard_bimodal_density.png]]
     #+CAPTION: pCN
     #+NAME: fig:hist_pCN
     [[./figures/pCN_bimodal_density.png]]

     The autocorrelation decays for all samplers: [[fig:ac_normal]], [[fig:ac_bimodal]]. However, the pCN doens't
     do nearly as well as expected. This could be the consequence of the awkward
     formulation of the potential or a bad prior.

     A peculiar thing about the decorrelation of the pCN sampling process is that
     it somehow is tied to the number of samples, compare [[fig:ac_pCN_1000]] and [[fig:ac_pCN_2000]].
     Is this a bug or a misunderstanding of the autocorrelation function? 

     #+CAPTION: AC of standard normal. All samplers decorrelate quickly
     #+NAME: fig:ac_normal
     [[./figures/analytic_standard_rw_pCN_normal.png]]

     #+CAPTION: AC of bimodal distribution. pCN takes forever to decorrelate
     #+NAME: fig:ac_bimodal
     [[./figures/analytic_standard_rw_pCN_bimodal_20000.png]]

     #+CAPTION: AC of bimodal distribution.
     #+NAME: fig:ac_pCN_1000
     [[./figures/analytic_standard_rw_pCN_bimodal_1000.png]]

     #+CAPTION: AC of bimodal distribution.
     #+NAME: fig:ac_pCN_2000
     [[./figures/analytic_standard_rw_pCN_bimodal_2000.png]]
     

*** Bayesian inverse problem for $\G{u} = \langle g,u \rangle$
*** Bayesian inverse problem for $\G{u} = g (u + \beta u^3)$
*** Geophysics example


#+BIBLIOGRAPHY: ../papers/inverse_problems plain
